{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "import pandas as pd\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos o dataset (previamente baixado do Kaggle) e exibimos suas 5 primeiras linhas \n",
    "#com a função \"head\"\n",
    "data = pd.read_csv('input/Skyserver_SQL2_27_2018 6_51_39 PM.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#já a função unstitch separa os atributos da label, fazendo o oposto da função stitch\n",
    "def unstitch(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    return df[0].values.tolist(), df[1].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separamos o dataset original nos conjuntos de treino, validação e teste\n",
    "#o conjunto de treino ficou com 60% dos exemplos, enquanto o de validação ficou com 20%,\n",
    "#e o de testes também com 20%\n",
    "train, test = train_test_split(data, test_size=0.2, shuffle=True)\n",
    "train, val = train_test_split(train, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    #em __init__ definimos cada uma das camadas\n",
    "    def __init__(self, activate_dropout=False, weightDecay=0):\n",
    "        super(Net, self).__init__()\n",
    "        #fc1 corresponde à camada de entrada e terá 20 neurônios\n",
    "        self.fc1 = nn.Linear(2, 20)\n",
    "        #a função de ativação utilizada para cada neurônio é tanh\n",
    "        self.rl1 = nn.Tanh()\n",
    "        # sigmoid ou relu\n",
    "        #fc2 corresponde à camada oculta, também tendo 20 neurônios\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.rl2 = nn.Tanh()\n",
    "        #já fc3 corresponde à cama de saída, com apenas 3 neurônios, visto que lidamos com\n",
    "        #3 classes na classificação\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        #a função de saída será softmax, recomendado para casos de classificação com múltiplas\n",
    "        #classes\n",
    "        # sigmoid\n",
    "        self.smout = nn.Sigmoid()\n",
    "        \n",
    "        #a função de loss utilizada é L1 (MAE, ou erro absoluto médio)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        #o otimizador selecionado foi Adam\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        self.activate_dropout=activate_dropout\n",
    "    def forward(self, x):\n",
    "        #o PyTorch requer que o forward propagation seja declarado para cada camada\n",
    "        x = self.fc1(x)\n",
    "        if self.activate_dropout:\n",
    "            x = self.do1(x)\n",
    "        x = self.rl1(x)\n",
    "        x = self.fc2(x)\n",
    "        if self.activate_dropout:\n",
    "            x = self.do2(x)\n",
    "        x = self.rl2(x)\n",
    "        x = self.fc3(x)\n",
    "        if self.activate_dropout:\n",
    "            x = self.do3(x)\n",
    "        x = self.smout(x)\n",
    "        return x\n",
    "    def loss(self, loss_set):\n",
    "        #a função de loss é definida\n",
    "        inputs, labels = unstitch(loss_set)\n",
    "        inputs = Variable(torch.FloatTensor(inputs))\n",
    "        labels = Variable(torch.FloatTensor(labels))\n",
    "        result = self(inputs)\n",
    "        loss = self.criterion(result, torch.max(labels, 1)[1])\n",
    "\n",
    "        return float(loss.item())\n",
    "    def train(self, numEpochs, train):\n",
    "        #função que realiza o treino da rede\n",
    "        #o treino inicia com a separação dos parâmetros e das labels\n",
    "        inputs, labels = unstitch(train)\n",
    "        # usar pra separa a reposta(label) do input\n",
    "        inputs = Variable(torch.FloatTensor(inputs))\n",
    "        labels = Variable(torch.FloatTensor(labels))\n",
    "\n",
    "        #em seguida, para cada época é realizado o forward propagation, e então o otimizador\n",
    "        #efetua o back propagation atualizando os pesos de cada neurônio\n",
    "        #os resultados de cada época são armazenados para exibição no TensorBoard\n",
    "        for epoch in range(numEpochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self(inputs)\n",
    "            self.loss = self.criterion(outputs, labels)\n",
    "            self.loss.backward()    \n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.store_results(epoch, train)\n",
    "            self.print_final_train_acc(epoch, train)\n",
    "    def accuracy(self, acc_set):\n",
    "        #definição do cálculo da acurácia\n",
    "        inputs, labels = unstitch(acc_set)\n",
    "        result = self(Variable(torch.FloatTensor(inputs)))\n",
    "        inputs_max = np.argmax(result.detach().cpu().numpy(), axis=1)\n",
    "        labels_max = np.argmax(np.array(labels), axis=1)\n",
    "        correct = np.sum(inputs_max == labels_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_RATE = 10\n",
    "STORE_RESULT = True\n",
    "NUM_EPOCHS = 5000\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT_RATE = 0.1\n",
    "ACTIVATE_DROPOUT = False\n",
    "WEIGHT_DECAY = 0\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "#executamos o treinamento, verificando o tempo levado para que ele ocorra\n",
    "t0=time.time()\n",
    "# writer = SummaryWriter(log_dir='runs3/arch_3layer_final')\n",
    "batchNet = Net(ACTIVATE_DROPOUT, WEIGHT_DECAY)\n",
    "batchNet.train(NUM_EPOCHS, train)\n",
    "t1=time.time()\n",
    "# parametros utilizadsoq ue deram certo\n",
    "\n",
    "#é criada uma linha para o arquivo CSV com os dados da execução atual\n",
    "grid_search_df = pd.DataFrame({'layer_type': [\"Linear\"],\n",
    "                               'layers': [\"(6, 20), (20, 20), (10, 2)\"],\n",
    "                               'num_trainable_params':['630'],\n",
    "                               'time': [str(round(t1-t0,3))],\n",
    "                               'activation': [\"TanH\"],\n",
    "                               'optimizer': [\"Adam\"],\n",
    "                               'optimizer_params': [\"actual weight_decay=\"+str(WEIGHT_DECAY)],\n",
    "                               'learning_rate': [\"actual learning_rate=\"+str(LEARNING_RATE)],\n",
    "                               'loss_func': [\"MSELoss\"],\n",
    "                               'activate_dropout': [str(ACTIVATE_DROPOUT)],\n",
    "                               'dropout_rate': [str(DROPOUT_RATE)],\n",
    "                               'num_epochs': [str(NUM_EPOCHS)],\n",
    "                               'batch_size': [str(BATCH_SIZE)],\n",
    "                               'train_accuracy': [str(round(batchNet.accuracy(train),4))],\n",
    "                               'val_accuracy': [str(round(batchNet.accuracy(val),4))]\n",
    "                               })\n",
    "\n",
    "#a acurácia de validação é exibida\n",
    "print(f\"Validation accuracy: {batchNet.accuracy(val)}\")\n",
    "print(\"-\"*10)\n",
    "\n",
    "# a linha do arquivo CSV é efetivamente inserida no arquivo\n",
    "file_df = pd.read_csv(\"grid_search.csv\")\n",
    "file_df = file_df.append(grid_search_df)\n",
    "file_df.to_csv(\"grid_search.csv\",index=False)\n",
    "# writer.close()\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#após finalizar a busca pelos hiperparâmetros ideais, realizamos a checagem da acurácia\n",
    "#para o conjunto de teste\n",
    "print(f\"Test accuracy: {batchNet.accuracy(test)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
